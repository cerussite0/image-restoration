{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Standard library imports\n",
    "import random\n",
    "import json\n",
    "# Third-party imports\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init as init\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torchmetrics.functional import peak_signal_noise_ratio\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs_lm = 10\n",
    "batch_size = 16\n",
    "num_epochs = 500\n",
    "checkpoint_dir=\"/home/cvpr_ug_2/abhinavinstructir/checkpoint\"\n",
    "\n",
    "############======== IMAGE MODEL =========############\n",
    "class SimpleGate(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x1, x2 = x.chunk(2, dim=1) # Split tensor into 2 equal parts along the channel dimension\n",
    "        return x1 * x2\n",
    "\n",
    "class NAFBlock(nn.Module):\n",
    "    def __init__(self, c, DW_Expand=2, FFN_Expand=2, drop_out_rate=0.): # expansion ratio for the depthwise convolution\n",
    "        super().__init__()\n",
    "        dw_channel = c * DW_Expand\n",
    "        self.conv1 = nn.Conv2d(in_channels=c, out_channels=dw_channel, kernel_size=1, padding=0, stride=1, groups=1, bias=True) # groups=1 means basic simple convolution (not grouped/depthwise)\n",
    "        self.conv2 = nn.Conv2d(in_channels=dw_channel, out_channels=dw_channel, kernel_size=3, padding=1, stride=1, groups=dw_channel,\n",
    "                               bias=True)\n",
    "        self.conv3 = nn.Conv2d(in_channels=dw_channel // 2, out_channels=c, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
    "\n",
    "        # Simplified Channel Attention\n",
    "        self.sca = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), # Takes an input feature map of shape (N, C, H, W), reduces each channel to a 1×1 spatial size by computing the avg over H×W --> (N, C, 1, 1)\n",
    "            nn.Conv2d(in_channels=dw_channel // 2, out_channels=dw_channel // 2, kernel_size=1, padding=0, stride=1,\n",
    "                      groups=1, bias=True),\n",
    "        )\n",
    "\n",
    "        # SimpleGate\n",
    "        self.sg = SimpleGate()\n",
    "\n",
    "        ffn_channel = FFN_Expand * c\n",
    "        self.conv4 = nn.Conv2d(in_channels=c, out_channels=ffn_channel, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
    "        self.conv5 = nn.Conv2d(in_channels=ffn_channel // 2, out_channels=c, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
    "\n",
    "        self.norm1 = LayerNorm2d(c)\n",
    "        self.norm2 = LayerNorm2d(c)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()\n",
    "        self.dropout2 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()\n",
    "\n",
    "        self.beta = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
    "        self.gamma = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = inp\n",
    "\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.sg(x)\n",
    "        x = x * self.sca(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        y = inp + x * self.beta\n",
    "\n",
    "        x = self.conv4(self.norm2(y))\n",
    "        x = self.sg(x)\n",
    "        x = self.conv5(x)\n",
    "\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        return y + x * self.gamma\n",
    "\n",
    "\n",
    "class NAFNet(nn.Module):\n",
    "    def __init__(self, img_channel=3, width=16, middle_blk_num=1, enc_blk_nums=[], dec_blk_nums=[]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.intro = nn.Conv2d(in_channels=img_channel, out_channels=width, kernel_size=3, padding=1, stride=1, groups=1,\n",
    "                              bias=True)\n",
    "        self.ending = nn.Conv2d(in_channels=width, out_channels=img_channel, kernel_size=3, padding=1, stride=1, groups=1,\n",
    "                              bias=True)\n",
    "\n",
    "        self.encoders = nn.ModuleList()\n",
    "        self.decoders = nn.ModuleList()\n",
    "        self.middle_blks = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "\n",
    "        chan = width\n",
    "        for num in enc_blk_nums: \n",
    "            self.encoders.append(\n",
    "                nn.Sequential(\n",
    "                    *[NAFBlock(chan) for _ in range(num)]\n",
    "                )\n",
    "            )\n",
    "            self.downs.append(\n",
    "                nn.Conv2d(chan, 2*chan, 2, 2)\n",
    "            )\n",
    "            chan = chan * 2\n",
    "\n",
    "        self.middle_blks = \\\n",
    "            nn.Sequential(\n",
    "                *[NAFBlock(chan) for _ in range(middle_blk_num)]\n",
    "            )\n",
    "\n",
    "        for num in dec_blk_nums:\n",
    "            self.ups.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(chan, chan * 2, 1, bias=False),\n",
    "                    nn.PixelShuffle(2) #Rearranges feature maps from channels into spatial resolution:\n",
    "                    #Input: shape (B, chan*2, H, W) Output: (B, chan/2, 2H, 2W)\n",
    "                )\n",
    "            )\n",
    "            chan = chan // 2\n",
    "            self.decoders.append(\n",
    "                nn.Sequential(\n",
    "                    *[NAFBlock(chan) for _ in range(num)]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.padder_size = 2 ** len(self.encoders)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        B, C, H, W = inp.shape\n",
    "        inp = self.check_image_size(inp)\n",
    "\n",
    "        x = self.intro(inp)\n",
    "\n",
    "        encs = []\n",
    "\n",
    "        for encoder, down in zip(self.encoders, self.downs):\n",
    "            x = encoder(x)\n",
    "            encs.append(x)\n",
    "            x = down(x)\n",
    "\n",
    "        x = self.middle_blks(x)\n",
    "\n",
    "        for decoder, up, enc_skip in zip(self.decoders, self.ups, encs[::-1]):\n",
    "            x = up(x)\n",
    "            x = x + enc_skip\n",
    "            x = decoder(x)\n",
    "\n",
    "        x = self.ending(x)\n",
    "        x = x + inp\n",
    "\n",
    "        return x[:, :, :H, :W]\n",
    "\n",
    "    def check_image_size(self, x):\n",
    "        _, _, h, w = x.size()\n",
    "        mod_pad_h = (self.padder_size - h % self.padder_size) % self.padder_size\n",
    "        mod_pad_w = (self.padder_size - w % self.padder_size) % self.padder_size\n",
    "        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Local_Base():\n",
    "    def convert(self, *args, train_size, **kwargs):\n",
    "        replace_layers(self, *args, train_size=train_size, **kwargs)\n",
    "        imgs = torch.rand(train_size)\n",
    "        with torch.no_grad():\n",
    "            self.forward(imgs)\n",
    "\n",
    "class NAFNetLocal(Local_Base, NAFNet):\n",
    "    def __init__(self, *args, train_size=(1, 3, 256, 256), fast_imp=False, **kwargs):\n",
    "        Local_Base.__init__(self)\n",
    "        NAFNet.__init__(self, *args, **kwargs)\n",
    "\n",
    "        N, C, H, W = train_size\n",
    "        base_size = (int(H * 1.5), int(W * 1.5))\n",
    "\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            self.convert(base_size=base_size, train_size=train_size, fast_imp=fast_imp)\n",
    "\n",
    "\n",
    "def create_nafnet(input_channels = 3, width = 32, enc_blks = [2, 2, 4, 8], middle_blk_num = 12, dec_blks = [2, 2, 2, 2]):\n",
    "    \"\"\"\n",
    "    Create Nafnet model\n",
    "    https://github.com/megvii-research/NAFNet/blob/main/options/test/SIDD/NAFNet-width32.yml\n",
    "    \"\"\"\n",
    "    \n",
    "    net = NAFNet(img_channel=input_channels, width=width, middle_blk_num=middle_blk_num,\n",
    "                      enc_blk_nums=enc_blks, dec_blk_nums=dec_blks)\n",
    "    \n",
    "    return net\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Copyright (c) 2022 megvii-model. All Rights Reserved.\n",
    "# ------------------------------------------------------------------------\n",
    "# Source: https://github.com/megvii-research/NAFNet\n",
    "\n",
    "\n",
    "\n",
    "class LayerNormFunction(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, weight, bias, eps):\n",
    "        ctx.eps = eps\n",
    "        N, C, H, W = x.size()\n",
    "        mu = x.mean(1, keepdim=True)\n",
    "        var = (x - mu).pow(2).mean(1, keepdim=True)\n",
    "        y = (x - mu) / (var + eps).sqrt()\n",
    "        ctx.save_for_backward(y, var, weight)\n",
    "        y = weight.view(1, C, 1, 1) * y + bias.view(1, C, 1, 1)\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        eps = ctx.eps\n",
    "\n",
    "        N, C, H, W = grad_output.size()\n",
    "        y, var, weight = ctx.saved_variables\n",
    "        g = grad_output * weight.view(1, C, 1, 1)\n",
    "        mean_g = g.mean(dim=1, keepdim=True)\n",
    "\n",
    "        mean_gy = (g * y).mean(dim=1, keepdim=True)\n",
    "        gx = 1. / torch.sqrt(var + eps) * (g - y * mean_gy - mean_g)\n",
    "        return gx, (grad_output * y).sum(dim=3).sum(dim=2).sum(dim=0), grad_output.sum(dim=3).sum(dim=2).sum(dim=0), None\n",
    "\n",
    "class LayerNorm2d(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, eps=1e-6):\n",
    "        super(LayerNorm2d, self).__init__()\n",
    "        self.register_parameter('weight', nn.Parameter(torch.ones(channels)))\n",
    "        self.register_parameter('bias', nn.Parameter(torch.zeros(channels)))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return LayerNormFunction.apply(x, self.weight, self.bias, self.eps)\n",
    "    \n",
    "\n",
    "\n",
    "class AvgPool2d(nn.Module):\n",
    "    def __init__(self, kernel_size=None, base_size=None, auto_pad=True, fast_imp=False, train_size=None):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.base_size = base_size\n",
    "        self.auto_pad = auto_pad\n",
    "\n",
    "        # only used for fast implementation\n",
    "        self.fast_imp = fast_imp\n",
    "        self.rs = [5, 4, 3, 2, 1]\n",
    "        self.max_r1 = self.rs[0]\n",
    "        self.max_r2 = self.rs[0]\n",
    "        self.train_size = train_size\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'kernel_size={}, base_size={}, stride={}, fast_imp={}'.format(\n",
    "            self.kernel_size, self.base_size, self.kernel_size, self.fast_imp\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.kernel_size is None and self.base_size:\n",
    "            train_size = self.train_size\n",
    "            if isinstance(self.base_size, int):\n",
    "                self.base_size = (self.base_size, self.base_size)\n",
    "            self.kernel_size = list(self.base_size)\n",
    "            self.kernel_size[0] = x.shape[2] * self.base_size[0] // train_size[-2]\n",
    "            self.kernel_size[1] = x.shape[3] * self.base_size[1] // train_size[-1]\n",
    "\n",
    "            # only used for fast implementation\n",
    "            self.max_r1 = max(1, self.rs[0] * x.shape[2] // train_size[-2])\n",
    "            self.max_r2 = max(1, self.rs[0] * x.shape[3] // train_size[-1])\n",
    "\n",
    "        if self.kernel_size[0] >= x.size(-2) and self.kernel_size[1] >= x.size(-1):\n",
    "            return F.adaptive_avg_pool2d(x, 1)\n",
    "\n",
    "        if self.fast_imp:  # Non-equivalent implementation but faster\n",
    "            h, w = x.shape[2:]\n",
    "            if self.kernel_size[0] >= h and self.kernel_size[1] >= w:\n",
    "                out = F.adaptive_avg_pool2d(x, 1)\n",
    "            else:\n",
    "                r1 = [r for r in self.rs if h % r == 0][0]\n",
    "                r2 = [r for r in self.rs if w % r == 0][0]\n",
    "                # reduction_constraint\n",
    "                r1 = min(self.max_r1, r1)\n",
    "                r2 = min(self.max_r2, r2)\n",
    "                s = x[:, :, ::r1, ::r2].cumsum(dim=-1).cumsum(dim=-2)\n",
    "                n, c, h, w = s.shape\n",
    "                k1, k2 = min(h - 1, self.kernel_size[0] // r1), min(w - 1, self.kernel_size[1] // r2)\n",
    "                out = (s[:, :, :-k1, :-k2] - s[:, :, :-k1, k2:] - s[:, :, k1:, :-k2] + s[:, :, k1:, k2:]) / (k1 * k2)\n",
    "                out = torch.nn.functional.interpolate(out, scale_factor=(r1, r2))\n",
    "        else:\n",
    "            n, c, h, w = x.shape\n",
    "            s = x.cumsum(dim=-1).cumsum_(dim=-2)\n",
    "            s = torch.nn.functional.pad(s, (1, 0, 1, 0))  # pad 0 for convenience\n",
    "            k1, k2 = min(h, self.kernel_size[0]), min(w, self.kernel_size[1])\n",
    "            s1, s2, s3, s4 = s[:, :, :-k1, :-k2], s[:, :, :-k1, k2:], s[:, :, k1:, :-k2], s[:, :, k1:, k2:]\n",
    "            out = s4 + s1 - s2 - s3\n",
    "            out = out / (k1 * k2)\n",
    "\n",
    "        if self.auto_pad:\n",
    "            n, c, h, w = x.shape\n",
    "            _h, _w = out.shape[2:]\n",
    "            # print(x.shape, self.kernel_size)\n",
    "            pad2d = ((w - _w) // 2, (w - _w + 1) // 2, (h - _h) // 2, (h - _h + 1) // 2)\n",
    "            out = torch.nn.functional.pad(out, pad2d, mode='replicate')\n",
    "\n",
    "        return out\n",
    "\n",
    "def replace_layers(model, base_size, train_size, fast_imp, **kwargs):\n",
    "    for n, m in model.named_children():\n",
    "        if len(list(m.children())) > 0:\n",
    "            ## compound module, go inside it\n",
    "            replace_layers(m, base_size, train_size, fast_imp, **kwargs)\n",
    "\n",
    "        if isinstance(m, nn.AdaptiveAvgPool2d):\n",
    "            pool = AvgPool2d(base_size=base_size, fast_imp=fast_imp, train_size=train_size)\n",
    "            assert m.output_size == 1\n",
    "            setattr(model, n, pool)\n",
    "\n",
    "class ICB(nn.Module):\n",
    "    \"\"\"\n",
    "    Instruction Condition Block (ICB)\n",
    "    Paper Section 3.3\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim, text_dim=768):\n",
    "        super(ICB, self).__init__()\n",
    "        self.fc    = nn.Linear(text_dim, feature_dim)\n",
    "        self.block = NAFBlock(feature_dim)\n",
    "        self.beta  = nn.Parameter(torch.zeros((1, feature_dim, 1, 1)), requires_grad=True)\n",
    "        self.gamma = nn.Parameter(torch.zeros((1, feature_dim, 1, 1)), requires_grad=True)\n",
    "\n",
    "    def forward(self, x, text_embedding):\n",
    "        gating_factors = torch.sigmoid(self.fc(text_embedding))\n",
    "        gating_factors = gating_factors.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        f = x * self.gamma + self.beta  # 1) learned feature scaling/modulation\n",
    "        f = f * gating_factors          # 2) (soft) feature routing based on text\n",
    "        f = self.block(f)               # 3) block feature enhancement\n",
    "        return f + x\n",
    "\n",
    "\n",
    "class InstructIR(nn.Module):\n",
    "    \"\"\"\n",
    "    InstructIR model using NAFNet (ECCV 2022) as backbone.\n",
    "    The model takes as input an RGB image and a text embedding (encoded instruction).\n",
    "    Described in Paper Section 3.3\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_channel=3, width=16, middle_blk_num=1, enc_blk_nums=[], dec_blk_nums=[], txtdim=768):\n",
    "        super().__init__()\n",
    "\n",
    "        self.intro  = nn.Conv2d(in_channels=img_channel, out_channels=width, kernel_size=3, padding=1, stride=1, groups=1,\n",
    "                              bias=True)\n",
    "        self.ending = nn.Conv2d(in_channels=width, out_channels=img_channel, kernel_size=3, padding=1, stride=1, groups=1,\n",
    "                              bias=True)\n",
    "\n",
    "        self.encoders    = nn.ModuleList()\n",
    "        self.decoders    = nn.ModuleList()\n",
    "        self.middle_blks = nn.ModuleList()\n",
    "        self.ups         = nn.ModuleList()\n",
    "        self.downs       = nn.ModuleList()\n",
    "        self.enc_cond    = nn.ModuleList()\n",
    "        self.dec_cond    = nn.ModuleList()\n",
    "\n",
    "        chan = width\n",
    "        for num in enc_blk_nums:\n",
    "            self.encoders.append(\n",
    "                nn.Sequential(\n",
    "                    *[NAFBlock(chan) for _ in range(num)]\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            self.enc_cond.append(ICB(chan,txtdim))\n",
    "\n",
    "            self.downs.append(\n",
    "                nn.Conv2d(chan, 2*chan, 2, 2)\n",
    "            )\n",
    "            chan = chan * 2\n",
    "\n",
    "        self.middle_blks = nn.Sequential(\n",
    "                *[NAFBlock(chan) for _ in range(middle_blk_num)]\n",
    "            )\n",
    "\n",
    "        for num in dec_blk_nums:\n",
    "            self.ups.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(chan, chan * 2, 1, bias=False),\n",
    "                    nn.PixelShuffle(2)\n",
    "                )\n",
    "            )\n",
    "            chan = chan // 2\n",
    "            self.decoders.append(  \n",
    "                nn.Sequential(\n",
    "                    *[NAFBlock(chan) for _ in range(num)]\n",
    "                )\n",
    "            )\n",
    "            # Add text embedding as modulation\n",
    "            self.dec_cond.append(ICB(chan,txtdim))\n",
    "\n",
    "        self.padder_size = 2 ** len(self.encoders)\n",
    "\n",
    "    def forward(self, inp, txtembd):\n",
    "        B, C, H, W = inp.shape\n",
    "        inp = self.check_image_size(inp)\n",
    "\n",
    "        x = self.intro(inp)\n",
    "        encs = []\n",
    "\n",
    "        for encoder, enc_mod, down in zip(self.encoders, self.enc_cond, self.downs):\n",
    "            x = encoder(x)\n",
    "            x = enc_mod(x,txtembd)\n",
    "            encs.append(x)\n",
    "            x = down(x)\n",
    "\n",
    "        x = self.middle_blks(x)\n",
    "\n",
    "        for decoder, up, enc_skip, dec_mod in zip(self.decoders, self.ups, encs[::-1], self.dec_cond):\n",
    "            x = up(x)\n",
    "            x = x + enc_skip\n",
    "            x = decoder(x)\n",
    "            x = dec_mod(x,txtembd)\n",
    "\n",
    "        x = self.ending(x)\n",
    "        x = x + inp\n",
    "\n",
    "        return x[:, :, :H, :W]\n",
    "\n",
    "    def check_image_size(self, x):\n",
    "        _, _, h, w = x.size()\n",
    "        mod_pad_h = (self.padder_size - h % self.padder_size) % self.padder_size\n",
    "        mod_pad_w = (self.padder_size - w % self.padder_size) % self.padder_size\n",
    "        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h))\n",
    "        return x\n",
    "\n",
    "\n",
    "def create_model(input_channels = 3, width = 32, enc_blks = [2, 2, 4, 8], middle_blk_num = 12, dec_blks = [2, 2, 2, 2], txtdim=768):\n",
    "\n",
    "    net = InstructIR(img_channel=input_channels, width=width, middle_blk_num=middle_blk_num,\n",
    "                      enc_blk_nums=enc_blks, dec_blk_nums=dec_blks, txtdim=txtdim)\n",
    "\n",
    "    return net\n",
    "\n",
    "############======== LANGUAGE MODEL =========############\n",
    "\n",
    "\n",
    "# Models that use mean pooling\n",
    "POOL_MODELS = {\"sentence-transformers/all-MiniLM-L6-v2\", \"TaylorAI/bge-micro-v2\"}\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, model='TaylorAI/bge-micro-v2'):\n",
    "        super(LanguageModel, self).__init__()\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
    "        self.er = AutoTokenizer.from_pretrained(model)\n",
    "        self.model = AutoModel.from_pretrained(model).to(device)\n",
    "        self.model_name = model\n",
    "        # Remove the CLIP vision tower\n",
    "        if \"clip\" in self.model_name:\n",
    "            self.model.vision_model = None\n",
    "            \n",
    "        # Freeze the pre-trained parameters (very important)\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Make sure to set evaluation mode (also important)\n",
    "        self.model.eval()\n",
    "\n",
    "    def forward(self, text_batch):\n",
    "        inputs = self.tokenizer(text_batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad(): # Ensure no gradients are computed for this forward pass\n",
    "\n",
    "            if \"clip\" in self.model_name:\n",
    "                sentence_embedding = self.model.get_text_features(**inputs)\n",
    "                return sentence_embedding\n",
    "\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        if any(model in self.model_name for model in POOL_MODELS):\n",
    "            sentence_embeddings = mean_pooling(outputs, inputs['attention_mask'])\n",
    "            # Normalize embeddings\n",
    "            sentence_embedding = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        else:\n",
    "            sentence_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        return sentence_embedding\n",
    "    \n",
    "\n",
    "class LMHead(nn.Module):\n",
    "    def __init__(self, embedding_dim=384, hidden_dim=256, num_classes=4):\n",
    "        super(LMHead, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        #self.gelu = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embd = self.fc1(x)\n",
    "        embd = F.normalize(embd, p=2, dim=1)\n",
    "        deg_pred = self.fc2(embd)\n",
    "        return embd, deg_pred\n",
    "\n",
    "\n",
    "###############=============PROMPT DATALOADER============##############\n",
    "class TaskPromptDataset(Dataset):\n",
    "    \"\"\"Dataset for prompts from a single task\"\"\"\n",
    "    def __init__(self, prompts, task_id):\n",
    "        self.samples = [(prompt.strip(), task_id) for prompt in prompts]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text, label = self.samples[idx]\n",
    "        return text, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "class MultiTaskPromptLoader:\n",
    "    \"\"\"Dictionary-style loader that provides batched prompts per task\"\"\"\n",
    "    def __init__(self, json_path, batch_size=16):\n",
    "        with open(json_path, 'r') as f:\n",
    "            raw_data = json.load(f)    \n",
    "        \n",
    "        # Map between task names, JSON keys, and task IDs\n",
    "        self.task_config = {\n",
    "            'blur': {'json_key': 'blur', 'id': 2},\n",
    "            'haze': {'json_key': 'haze', 'id': 4},\n",
    "            'noise': {'json_key': 'noise', 'id': 0},\n",
    "            'lol': {'json_key': 'lol', 'id': 3},\n",
    "            'rain': {'json_key': 'rain', 'id': 1}\n",
    "        }\n",
    "        \n",
    "        self.loaders = {}\n",
    "        for task_name, config in self.task_config.items():\n",
    "            # Get prompts for this task from JSON\n",
    "            prompts = raw_data.get(config['json_key'], [])\n",
    "            \n",
    "            # Create dataset and loader\n",
    "            dataset = TaskPromptDataset(prompts, config['id'])\n",
    "            loader = DataLoader(\n",
    "                dataset, \n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=4,\n",
    "                collate_fn=self._collate_fn,\n",
    "                pin_memory=True,\n",
    "                drop_last=True\n",
    "            )\n",
    "            self.loaders[task_name] = loader\n",
    "            \n",
    "        self.iterators = {task: iter(loader) for task, loader in self.loaders.items()}\n",
    "\n",
    "    def _collate_fn(self, batch):\n",
    "        \"\"\"Custom collate to handle text data\"\"\"\n",
    "        texts, labels = zip(*batch)\n",
    "        return list(texts), torch.stack(labels)\n",
    "\n",
    "    def get_batch(self, key):\n",
    "        \"\"\"Get a batch from specified task\"\"\"\n",
    "        try:\n",
    "            return next(self.iterators[key])\n",
    "        except StopIteration:\n",
    "            self.iterators[key] = iter(self.loaders[key])\n",
    "            return next(self.iterators[key])\n",
    "        \n",
    "        \n",
    "###############=============IMAGE DATALOADER============###############\n",
    "class PairedTransform:\n",
    "    \"\"\"Handles paired image transformations\"\"\"\n",
    "    def __init__(self, size=(256, 256)):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, input_img, target_img):\n",
    "        # Resize if smaller than target size\n",
    "        if input_img.height < self.size[0] or input_img.width < self.size[1]:\n",
    "            input_img = TF.resize(input_img, self.size)\n",
    "            target_img = TF.resize(target_img, self.size)\n",
    "        else:\n",
    "            # Random crop\n",
    "            i, j, h, w = transforms.RandomCrop.get_params(input_img, self.size)\n",
    "            input_img = TF.crop(input_img, i, j, h, w)\n",
    "            target_img = TF.crop(target_img, i, j, h, w)\n",
    "\n",
    "        # Random horizontal flip\n",
    "        if torch.rand(1) < 0.5:\n",
    "            input_img = TF.hflip(input_img)\n",
    "            target_img = TF.hflip(target_img)\n",
    "\n",
    "        # Random vertical flip\n",
    "        if torch.rand(1) < 0.5:\n",
    "            input_img = TF.vflip(input_img)\n",
    "            target_img = TF.vflip(target_img)\n",
    "\n",
    "        return TF.to_tensor(input_img), TF.to_tensor(target_img)\n",
    "\n",
    "class TaskDataset(Dataset):\n",
    "    \"\"\"Dataset for a single task\"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.input_dir = os.path.join(root_dir, 'input')\n",
    "        self.target_dir = os.path.join(root_dir, 'target')\n",
    "        self.filenames = [f for f in os.listdir(self.input_dir) \n",
    "                         if os.path.isfile(os.path.join(self.input_dir, f))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_img = Image.open(os.path.join(self.input_dir, self.filenames[idx])).convert('RGB')\n",
    "        target_img = Image.open(os.path.join(self.target_dir, self.filenames[idx])).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            return self.transform(input_img, target_img)\n",
    "        return TF.to_tensor(input_img), TF.to_tensor(target_img)\n",
    "\n",
    "class MultiTaskLoader:\n",
    "    \"\"\"Dictionary-style loader that provides access to all tasks\"\"\"\n",
    "    def __init__(self, root_dir, batch_size=16, transform=None):\n",
    "        self.tasks = {\n",
    "            'blur': TaskDataset(os.path.join(root_dir, 'deblurring_dataset'), transform),\n",
    "            'haze': TaskDataset(os.path.join(root_dir, 'dehazing_dataset'), transform),\n",
    "            'lol': TaskDataset(os.path.join(root_dir, 'lol_dataset'), transform),\n",
    "            'noise': TaskDataset(os.path.join(root_dir, 'noise_dataset'), transform),\n",
    "            'rain': TaskDataset(os.path.join(root_dir, 'rainy_image_dataset'), transform)\n",
    "        }\n",
    "        \n",
    "        self.loaders = {\n",
    "            task: DataLoader(dataset, batch_size=batch_size, \n",
    "                            shuffle=True, num_workers=4, pin_memory=True,drop_last=True)\n",
    "            for task, dataset in self.tasks.items()\n",
    "        }\n",
    "        self.iterators = {task: iter(loader) for task, loader in self.loaders.items()}\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.loaders[key]\n",
    "\n",
    "    def get_batch(self, key):\n",
    "        \"\"\"Get a batch from specified task\"\"\"\n",
    "        try:\n",
    "            return next(self.iterators[key])\n",
    "        except StopIteration:\n",
    "            self.iterators[key] = iter(self.loaders[key])\n",
    "            return next(self.iterators[key])\n",
    "        \n",
    "\n",
    "######################====== ACCURACY CALCULATION ======#############################\n",
    "def calculate_accuracy(prompt_loader, language_model, lm_head, device):\n",
    "    \"\"\"Calculate classification accuracy across all tasks\"\"\"\n",
    "    lm_head.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Iterate through all tasks\n",
    "        for task_name, loader in prompt_loader.loaders.items():\n",
    "            # Create fresh iterator for each task\n",
    "            task_iterator = iter(loader)\n",
    "            \n",
    "            # Process all batches for this task\n",
    "            while True:\n",
    "                try:\n",
    "                    texts, labels = next(task_iterator)\n",
    "                    labels = labels.to(device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    embeddings = language_model(texts)\n",
    "                    _, logits = lm_head(embeddings)\n",
    "                    \n",
    "                    # Calculate accuracy\n",
    "                    preds = torch.argmax(logits, dim=1)\n",
    "                    total_correct += (preds == labels).sum().item()\n",
    "                    total_samples += labels.shape[0]\n",
    "                    \n",
    "                except StopIteration:\n",
    "                    break  # Move to next task\n",
    "    \n",
    "    accuracy = (total_correct / total_samples) * 100 if total_samples > 0 else 0.0\n",
    "    return accuracy\n",
    "\n",
    "######################====== PSNR CALCULATION ======#############################\n",
    "# Fixed prompts for evaluation\n",
    "FIXED_PROMPTS = {\n",
    "    'noise': [\"Help me with my picture, it's full of tiny spots.\"],\n",
    "    'blur': [\"Please, clean up this blurry photo.\"],\n",
    "    'rain': [\"Remove the streaks of falling rain from my photo.\"],\n",
    "    'lol': [\"Brighten the dark regions in this image without overexposing the highlights.\"],\n",
    "    'haze': [\"Remove the atmospheric haze from this image.\"]\n",
    "}\n",
    "\n",
    "def evaluate_model(image_model, lm_head, image_loader, device):\n",
    "    \"\"\"Calculate PSNR across all tasks using fixed prompts\"\"\"\n",
    "    image_model.eval()\n",
    "    lm_head.eval()\n",
    "    \n",
    "    psnr_results = []\n",
    "    table_data = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for task in image_loader.tasks.keys():\n",
    "            # Get fixed prompt embedding\n",
    "            texts = FIXED_PROMPTS[task]\n",
    "            embeddings = language_model(texts)\n",
    "            text_embd,_  = lm_head(embeddings)\n",
    "            \n",
    "            # Get sample batch\n",
    "            inputs, targets = image_loader.get_batch(task)\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Process images\n",
    "            outputs = image_model(inputs, text_embd)\n",
    "            \n",
    "            # Calculate PSNR\n",
    "            psnr = peak_signal_noise_ratio(outputs, targets).item()\n",
    "            psnr_results.append(psnr)\n",
    "            table_data.append([task, f\"{psnr:.2f} dB\"])\n",
    "    \n",
    "    # Print formatted table\n",
    "    print(\"\\n\" + tabulate(table_data, headers=[\"Task\", \"PSNR\"], tablefmt=\"grid\"))\n",
    "    return psnr_results\n",
    "\n",
    "def save_visuals(image_model, lm_head, image_loader, epoch, device):\n",
    "    \"\"\"Save 5x2 visualization grid\"\"\"\n",
    "    image_model.eval()\n",
    "    lm_head.eval()\n",
    "    \n",
    "    _, axs = plt.subplots(5, 2, figsize=(10, 25))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for row_idx, task in enumerate(image_loader.tasks.keys()):\n",
    "            # Get fixed prompt\n",
    "            texts = FIXED_PROMPTS[task]\n",
    "            embeddings = language_model(texts)\n",
    "            text_embd,_ = lm_head(embeddings)\n",
    "            \n",
    "            # Get sample images\n",
    "            inputs, targets = image_loader.get_batch(task)\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Process images\n",
    "            outputs = image_model(inputs, text_embd)\n",
    "            \n",
    "            # Convert to numpy\n",
    "            input_img = inputs[0].cpu().numpy().transpose(1, 2, 0)\n",
    "            output_img = outputs[0].cpu().numpy().transpose(1, 2, 0)\n",
    "            \n",
    "            # Plot\n",
    "            axs[row_idx, 0].imshow(input_img)\n",
    "            axs[row_idx, 0].set_title(f\"{task} Input\")\n",
    "            axs[row_idx, 1].imshow(output_img)\n",
    "            axs[row_idx, 1].set_title(f\"{task} Output\")\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    vis_path = os.path.join(VIS_DIR, f\"epoch_{epoch}.png\")\n",
    "    plt.savefig(vis_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved visualization to {vis_path}\")\n",
    "\n",
    "\n",
    "\n",
    "###############========Language Model Head TRAINING LOOP===========################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    transform = PairedTransform(size=(256, 256))\n",
    "    image_loader = MultiTaskLoader(root_dir='/home/cvpr_ug_2/abhinavDomainIR/Dataset', batch_size=batch_size,transform=transform)\n",
    "    prompt_loader = MultiTaskPromptLoader('/home/cvpr_ug_2/abhinavDomainIR/prompts.json', batch_size=batch_size)\n",
    "    ######################====== LANGUAGE MODEL INITIALIZATION ======#############################\n",
    "    LMODEL = 'TaylorAI/bge-micro-v2'\n",
    "    language_model = LanguageModel(model=LMODEL).to(device).eval() # Keep frozen\n",
    "    lm_head = LMHead(embedding_dim=384, hidden_dim=256, num_classes=5).to(device)\n",
    "    # Visualization config\n",
    "    VIS_DIR = os.path.join(checkpoint_dir, \"visualizations\")\n",
    "    os.makedirs(VIS_DIR, exist_ok=True)\n",
    "    ######################====== IMAGE MODEL INITIALIZATION ======#############################\n",
    "    image_model = create_model(input_channels = 3, width = 32, enc_blks = [2, 2, 4, 8], middle_blk_num = 4, dec_blks = [2, 2, 2, 2], txtdim=256)\n",
    "    image_model = image_model.to(device)\n",
    "    \n",
    "    ##############=======Parameter Count===========##########   \n",
    "    def count_trainable_parameters(model):\n",
    "        \"\"\"\n",
    "        Counts the number of trainable parameters in a PyTorch model.\n",
    "    \n",
    "        Args:\n",
    "            model: PyTorch model\n",
    "        \n",
    "        Returns:\n",
    "            total_params: Total number of trainable parameters\n",
    "        \"\"\"\n",
    "        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        return total_params\n",
    "    ################======== Count trainable parameters===========################\n",
    "    trainable_params = count_trainable_parameters(image_model)\n",
    "    print(f\"Total trainable parameters: {trainable_params:,}\")\n",
    "    ######################====== Loss and Optimizer ======#############################\n",
    "    criterion_class = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # LM Head-specific optimizer\n",
    "    optimizer_lm = torch.optim.AdamW(lm_head.parameters(), lr=5e-4)\n",
    "\n",
    "    print(\"Starting Stage 1: LM Head Training\")\n",
    "    for epoch in range(1,num_epochs_lm+1):\n",
    "        \n",
    "        lm_head.train()\n",
    "        for _ in range(1,2000,batch_size):\n",
    "        \n",
    "            # Random task selection\n",
    "            task = random.choice(['blur', 'haze', 'noise', 'lol', 'rain'])\n",
    "        \n",
    "            # Get batch\n",
    "            texts,labels = prompt_loader.get_batch(task)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass through frozen language model\n",
    "            with torch.no_grad():\n",
    "                embeddings = language_model(texts)\n",
    "\n",
    "             \n",
    "            # LM head forward\n",
    "            _, logits = lm_head(embeddings)\n",
    "            lm_loss = criterion_class(logits, labels)\n",
    "            \n",
    "            # Backpropagate\n",
    "            optimizer_lm.zero_grad()\n",
    "            lm_loss.backward()\n",
    "            optimizer_lm.step()            \n",
    "    \n",
    "        train_acc = calculate_accuracy(prompt_loader, language_model, lm_head, device) \n",
    "        print(f\"Epoch [{epoch}/{num_epochs_lm}] | \" f\"Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "    #################MAIN TRAINING LOOP#################\n",
    "\n",
    "\n",
    "    criterion_class = nn.CrossEntropyLoss()\n",
    "    criterion_image = nn.L1Loss()\n",
    "\n",
    "\n",
    "    optimizer_lm = optim.Adam(lm_head.parameters(), lr=1e-3)\n",
    "    \n",
    "    # Unified AdamW optimizer with learning rate 5e-4\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': lm_head.parameters()},\n",
    "        {'params': image_model.parameters()}\n",
    "        ], lr=5e-4)\n",
    "    \n",
    "    # Cosine annealing scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "\n",
    "    print(\"Starting Stage 2: Main Training\")\n",
    "    # Main training loop\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        lm_head.train()\n",
    "        image_model.train()\n",
    "    \n",
    "        for _ in range(0,2000,batch_size):\n",
    "\n",
    "            # Random task selection\n",
    "            task = random.choice(['blur', 'haze', 'noise', 'lol', 'rain'])\n",
    "        \n",
    "            # --- Language Model Head Training ---\n",
    "            texts,labels = prompt_loader.get_batch(task)\n",
    "            inputs, targets = image_loader.get_batch(task)\n",
    "\n",
    "            # Move data to device\n",
    "            labels = labels.to(device)\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # --- Forward passes ---\n",
    "            # Text embeddings (frozen language model)\n",
    "            with torch.no_grad():\n",
    "                embeddings = language_model(texts)\n",
    "            \n",
    "            # LM head forward\n",
    "            text_embd, logits = lm_head(embeddings)\n",
    "\n",
    "            # Image model forward\n",
    "            outputs = image_model(inputs, text_embd)\n",
    "            \n",
    "            # --- Loss calculation ---\n",
    "            loss_image = criterion_image(outputs, targets)\n",
    "            loss_class = criterion_class(logits, labels)\n",
    "            total_loss = loss_image + 0.3 * loss_class  # Combined loss\n",
    "\n",
    "            # --- Backward pass and optimize ---\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()    \n",
    "        \n",
    "        # Epoch evaluation\n",
    "        print(f\"\\nEpoch {epoch} Evaluation:\")\n",
    "    \n",
    "        # 1. LM Head Accuracy\n",
    "        train_acc = calculate_accuracy(prompt_loader, language_model, lm_head, device)\n",
    "        print(f\"Classification Accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "        # 2. PSNR Table\n",
    "        psnr_values = evaluate_model(image_model, lm_head, image_loader, device)\n",
    "    \n",
    "        if epoch % 50 == 0:\n",
    "            # Save models\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"joint_epoch_{epoch}.pth\")\n",
    "            torch.save({\n",
    "                'image_model': image_model.state_dict(),\n",
    "                'lm_head': lm_head.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict()\n",
    "            }, checkpoint_path)\n",
    "        \n",
    "            # Save visuals\n",
    "            save_visuals(image_model, lm_head, image_loader, epoch, device)\n",
    "            print(f\"Saved checkpoint and visuals for epoch {epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchmetrics.functional import peak_signal_noise_ratio, structural_similarity_index_measure\n",
    "from tabulate import tabulate\n",
    "import csv\n",
    "\n",
    "# Use the same device configuration as main\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load your existing components\n",
    "from main import (\n",
    "    LanguageModel,\n",
    "    LMHead,\n",
    "    create_model,\n",
    "    PairedTransform\n",
    ")\n",
    "\n",
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, input_dir, target_dir):\n",
    "        self.input_dir = input_dir\n",
    "        self.target_dir = target_dir\n",
    "        self.filenames = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f))]\n",
    "        \n",
    "        # Use same transforms as training\n",
    "        self.transform = PairedTransform(size=(256, 256))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_path = os.path.join(self.input_dir, self.filenames[idx])\n",
    "        target_path = os.path.join(self.target_dir, self.filenames[idx])\n",
    "        \n",
    "        input_img = Image.open(input_path).convert('RGB')\n",
    "        target_img = Image.open(target_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            input_tensor, target_tensor = self.transform(input_img, target_img)\n",
    "        else:\n",
    "            input_tensor = transforms.ToTensor()(input_img)\n",
    "            target_tensor = transforms.ToTensor()(target_img)\n",
    "            \n",
    "        return input_tensor, target_tensor, self.filenames[idx]\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, checkpoint_path):\n",
    "        # Initialize models\n",
    "        self.language_model = LanguageModel(model='TaylorAI/bge-micro-v2').to(device).eval()\n",
    "        self.lm_head = LMHead(embedding_dim=384, hidden_dim=256, num_classes=5).to(device)\n",
    "        image_model = create_model(input_channels = 3, width = 32, enc_blks = [2, 2, 4, 8], middle_blk_num = 4, dec_blks = [2, 2, 2, 2], txtdim=256)\n",
    "        image_model = image_model.to(device)\n",
    "        self.image_model = image_model\n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        self.image_model.load_state_dict(checkpoint['image_model'])\n",
    "        self.lm_head.load_state_dict(checkpoint['lm_head'])\n",
    "        self.image_model.eval()\n",
    "        self.lm_head.eval()\n",
    "        \n",
    "        # Fixed prompts from main\n",
    "        self.task_prompts = {\n",
    "            'denoising': [\"Help me with my picture, it's full of tiny spots.\"],\n",
    "            'deblurring': [\"Please, clean up this blurry photo.\"],\n",
    "            'deraining': [\"Remove the streaks of falling rain from my photo.\"],\n",
    "            'lowlight': [\"Brighten the dark regions in this image without overexposing the highlights.\"],\n",
    "            'dehazing': [\"Remove the atmospheric haze from this image.\"]\n",
    "        }\n",
    "\n",
    "    def get_task_embeddings(self, task_name):\n",
    "        texts = self.task_prompts[task_name]\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.language_model(texts)\n",
    "            text_embd, logits = self.lm_head(embeddings)\n",
    "        return text_embd, logits\n",
    "\n",
    "    def evaluate_dataset(self, input_dir, target_dir, task_name):\n",
    "        dataset = EvalDataset(input_dir, target_dir)\n",
    "        loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "        \n",
    "        psnr_values = []\n",
    "        ssim_values = []\n",
    "        \n",
    "        text_embd,_ = self.get_task_embeddings(task_name)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, filenames in loader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                outputs = self.image_model(inputs, text_embd)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                batch_psnr = peak_signal_noise_ratio(outputs, targets)\n",
    "                batch_ssim = structural_similarity_index_measure(outputs, targets)\n",
    "                \n",
    "                psnr_values.append(batch_psnr.item())\n",
    "                ssim_values.append(batch_ssim.item())\n",
    "                \n",
    "        return np.mean(psnr_values), np.mean(ssim_values)\n",
    "\n",
    "def generate_report(checkpoint_path, dataset_paths, output_file=\"evaluation_report.csv\"):\n",
    "    evaluator = ModelEvaluator(checkpoint_path)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for task_name, datasets in dataset_paths.items():\n",
    "        for dataset in datasets:\n",
    "            target_dir, input_dir, *extra = dataset\n",
    "            noise_level = extra[0] if extra else \"\"\n",
    "            \n",
    "            print(f\"Evaluating {task_name} - {noise_level if noise_level else ''}\")\n",
    "            \n",
    "            psnr, ssim = evaluator.evaluate_dataset(input_dir, target_dir, task_name)\n",
    "            \n",
    "            results.append({\n",
    "                'Task': task_name,\n",
    "                'Dataset': os.path.basename(input_dir),\n",
    "                'Noise Level': noise_level,\n",
    "                'PSNR': f\"{psnr:.2f}\",\n",
    "                'SSIM': f\"{ssim:.4f}\"\n",
    "            })\n",
    "    \n",
    "    # Save to CSV\n",
    "    with open(output_file, 'w') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=['Task', 'Dataset', 'Noise Level', 'PSNR', 'SSIM'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "    \n",
    "    # Print summary table\n",
    "    table_data = [[r['Task'], r['Dataset'], r['Noise Level'], r['PSNR'], r['SSIM']] for r in results]\n",
    "    print(\"\\nEvaluation Summary:\")\n",
    "    print(tabulate(table_data, headers=[\"Task\", \"Dataset\", \"Noise Level\", \"PSNR\", \"SSIM\"], tablefmt=\"grid\"))\n",
    "\n",
    "# Configuration\n",
    "# Dataset paths\n",
    "dataset_paths = {\n",
    "    \"denoising\": [\n",
    "        [\"/kaggle/input/image-restoration-test-data/test-data/denoising_testsets/CBSD68\", \"/kaggle/input/image-restoration-test-data/test-data/denoising_testsets/CBSD68_15\", \"15\"],\n",
    "        [\"/kaggle/input/image-restoration-test-data/test-data/denoising_testsets/CBSD68\", \"/kaggle/input/image-restoration-test-data/test-data/denoising_testsets/CBSD68_25\", \"25\"],\n",
    "        [\"/kaggle/input/image-restoration-test-data/test-data/denoising_testsets/CBSD68\", \"/kaggle/input/image-restoration-test-data/test-data/denoising_testsets/CBSD68_50\", \"50\"],\n",
    "        [\"/kaggle/input/image-restoration-test-data/test-data/denoising_testsets/Kodak24\", \"/kaggle/input/image-restoration-test-data/test-data/denoising_testsets/Kodak24_15\", \"15\"],\n",
    "        [\"/kaggle/input/image-restoration-test-data/test-data/denoising_testsets/Kodak24\", \"/kaggle/input/image-restoration-test-data/test-data/denoising_testsets/Kodak24_25\", \"25\"],\n",
    "        [\"/kaggle/input/image-restoration-test-data/test-data/denoising_testsets/Kodak24\", \"/kaggle/input/image-restoration-test-data/test-data/denoising_testsets/Kodak24_50\", \"50\"],\n",
    "        [\"/kaggle/input/image-restoration-test-data/test-data/denoising_testsets/urban100\", \"/kaggle/input/image-restoration-test-data/test-data/denoising_testsets/urban100_15\", \"15\"],\n",
    "        [\"/kaggle/input/image-restoration-test-data/test-data/denoising_testsets/urban100\", \"/kaggle/input/image-restoration-test-data/test-data/denoising_testsets/urban100_25\", \"25\"],\n",
    "        [\"/kaggle/input/image-restoration-test-data/test-data/denoising_testsets/urban100\", \"/kaggle/input/image-restoration-test-data/test-data/denoising_testsets/urban100_50\", \"50\"]\n",
    "    ],\n",
    "    \"deblurring\": [\n",
    "        [\"/kaggle/input/image-restoration-test-data/test-data/GoPro/target\", \"/kaggle/input/image-restoration-test-data/test-data/GoPro/input\", \"gopro\"]\n",
    "    ],\n",
    "    \"deraining\": [\n",
    "        [\"/kaggle/input/image-restoration-test-data/test-data/Rain100L/target\", \"/kaggle/input/image-restoration-test-data/test-data/Rain100L/input\", \"rain100l\"]\n",
    "    ],\n",
    "    \"lowlight\": [\n",
    "            [\"/kaggle/input/image-restoration-test-data/test-data/LOL/high\", \"/kaggle/input/image-restoration-test-data/test-data/LOL/low\", \"lol\"]\n",
    "    ],\n",
    "    \"dehazing\":[\n",
    "        [\"/kaggle/input/image-restoration-test-data/test-data/SOTS/GT\",\"/kaggle/input/image-restoration-test-data/test-data/SOTS/IN\",\"sots\"]\n",
    "    ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    checkpoint_path = \"/home/cvpr_ug_2/abhinavinstructir/checkpoint/joint_epoch_400.pth\"  # Update this\n",
    "    generate_report(checkpoint_path, dataset_paths)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2072626,
     "sourceId": 3440676,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6951142,
     "sourceId": 11143270,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8138580,
     "sourceId": 12866314,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8141874,
     "sourceId": 12871047,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
